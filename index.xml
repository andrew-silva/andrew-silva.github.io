<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Andrew Writing Things</title>
    <link>https://andrew-silva.github.io/</link>
    <description>Recent content on Andrew Writing Things</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 17 Feb 2025 00:51:47 -0500</lastBuildDate>
    <atom:link href="https://andrew-silva.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Learning a Personalized arXiv Feed</title>
      <link>https://andrew-silva.github.io/posts/personalized_paper_recommender/</link>
      <pubDate>Mon, 17 Feb 2025 00:51:47 -0500</pubDate>
      <guid>https://andrew-silva.github.io/posts/personalized_paper_recommender/</guid>
      <description>Keeping up with the latest on arXiv is hard ask, and requires somehow sifting through hundreds of papers every day. While I spent a long time relying on my network to surface important papers for my particular interests, this usually biases me towards a subset of authors or labs that are particularly adept at becoming popular on social media.
ArXiv-Sanity is a great solution to this problem, allowing you to find recommended papers based on TF-IDF vectors for papers.</description>
    </item>
    <item>
      <title>Investigating Relative Life Expectancy in the United States</title>
      <link>https://andrew-silva.github.io/posts/life_expectancy_visualizations/</link>
      <pubDate>Fri, 17 Jan 2025 14:48:00 -0500</pubDate>
      <guid>https://andrew-silva.github.io/posts/life_expectancy_visualizations/</guid>
      <description>I recently came across a compelling visualization of American healthcare costs vs. life expectancy, showing that the US is considerably outpacing other wealthy nations when it comes to spending on healthcare, but lagging behind when it comes to actually living longer.
This graphic paints a grim picture on two fronts. For one, wow US healthcare is expensive. But more importantly: why aren&amp;rsquo;t we getting anything for it? Why is American life expectancy so far behind other developed nations?</description>
    </item>
    <item>
      <title>Reading: Scaling Monosemanticity -- Extracting Interpretable Features from Claude 3 Sonnet</title>
      <link>https://andrew-silva.github.io/posts/scaling_monosemanticity_extracting_interpretable_features/</link>
      <pubDate>Wed, 22 May 2024 21:27:20 -0400</pubDate>
      <guid>https://andrew-silva.github.io/posts/scaling_monosemanticity_extracting_interpretable_features/</guid>
      <description>In a recent paper from Anthropic, the authors show that they can discover single features that correlate to concepts inside of a large language model (LLM). Specifically, the authors look at learning an autoencoder with an L1 norm penalty (to encourage sparsity) over the activations for a middle-layer in the network model, which learns to embed the activations of the LLM into a sparse feature space.
This sparsity means that they can take thousands or millions of dense feature activations and reduce all that noise down to just a few hundred high-magnitude features (on average, each token is represented by 300 independent features).</description>
    </item>
    <item>
      <title>Reading: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero Paper)</title>
      <link>https://andrew-silva.github.io/posts/muzero/</link>
      <pubDate>Wed, 23 Dec 2020 21:36:52 -0400</pubDate>
      <guid>https://andrew-silva.github.io/posts/muzero/</guid>
      <description>DeepMind’s Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model was officially published today in Nature, and is a reasonably short read covering a cool direction for future reinforcement learning (RL) research.
What is the paper trying to do? Following the incredible successes of AlphaGo and AlphaZero, this work introduces MuZero, which permits the extension of AlphaZero-like learning and performance to a new set of RL domains.</description>
    </item>
    <item>
      <title>Interactive Intelligence</title>
      <link>https://andrew-silva.github.io/posts/interactive_intelligence/</link>
      <pubDate>Sun, 20 Dec 2020 21:36:45 -0400</pubDate>
      <guid>https://andrew-silva.github.io/posts/interactive_intelligence/</guid>
      <description>Google DeepMind recently put a new 96 page paper on arXiv titled Imitating Interactive Intelligence. The paper actually has a few really cool things to contribute. Unfortunately, owing to its very intimidating length, it’s not easy to actually gain these insights. DeepMind have actually put up their own blog post on the work, but I didn’t really get a sense for what happened just by reading that post. They also released three videos showing what happened in the project (overview, training timelapse, and a demo).</description>
    </item>
  </channel>
</rss>
