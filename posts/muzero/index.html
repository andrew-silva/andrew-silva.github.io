<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reading: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero Paper) | Andrew Writing Things</title>
<meta name=keywords content><meta name=description content="DeepMind’s Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model was officially published today in Nature, and is a reasonably short read covering a cool direction for future reinforcement learning (RL) research.
What is the paper trying to do? Following the incredible successes of AlphaGo and AlphaZero, this work introduces MuZero, which permits the extension of AlphaZero-like learning and performance to a new set of RL domains."><meta name=author content><link rel=canonical href=https://andrew-silva.github.io/posts/muzero/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://andrew-silva.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://andrew-silva.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://andrew-silva.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://andrew-silva.github.io/apple-touch-icon.png><link rel=mask-icon href=https://andrew-silva.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B1B9JZ8WY8"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B1B9JZ8WY8",{anonymize_ip:!1})}</script><meta property="og:title" content="Reading: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero Paper)"><meta property="og:description" content="DeepMind’s Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model was officially published today in Nature, and is a reasonably short read covering a cool direction for future reinforcement learning (RL) research.
What is the paper trying to do? Following the incredible successes of AlphaGo and AlphaZero, this work introduces MuZero, which permits the extension of AlphaZero-like learning and performance to a new set of RL domains."><meta property="og:type" content="article"><meta property="og:url" content="https://andrew-silva.github.io/posts/muzero/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-23T21:36:52-04:00"><meta property="article:modified_time" content="2020-12-23T21:36:52-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reading: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero Paper)"><meta name=twitter:description content="DeepMind’s Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model was officially published today in Nature, and is a reasonably short read covering a cool direction for future reinforcement learning (RL) research.
What is the paper trying to do? Following the incredible successes of AlphaGo and AlphaZero, this work introduces MuZero, which permits the extension of AlphaZero-like learning and performance to a new set of RL domains."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://andrew-silva.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Reading: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero Paper)","item":"https://andrew-silva.github.io/posts/muzero/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reading: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero Paper)","name":"Reading: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero Paper)","description":"DeepMind’s Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model was officially published today in Nature, and is a reasonably short read covering a cool direction for future reinforcement learning (RL) research.\nWhat is the paper trying to do? Following the incredible successes of AlphaGo and AlphaZero, this work introduces MuZero, which permits the extension of AlphaZero-like learning and performance to a new set of RL domains.","keywords":[],"articleBody":"DeepMind’s Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model was officially published today in Nature, and is a reasonably short read covering a cool direction for future reinforcement learning (RL) research.\nWhat is the paper trying to do? Following the incredible successes of AlphaGo and AlphaZero, this work introduces MuZero, which permits the extension of AlphaZero-like learning and performance to a new set of RL domains.\nAlphaZero learned to master Go and Chess through the use of a simulator— RL agents use forward-simulation from each state in order to accurately estimate which move is the most valuable. So, from any given board state, the AlphaZero agent simulates many moves into the future, helping it to decide which move will yield the best long-term reward. Crucially, this only works if you have an accurate simulator. We can’t forward-simulate hundreds of steps for a task without a simulator.\nMuZero takes a step toward removing this restriction. The agent here learns to predict environment dynamics as they relate to the agent’s success, permitting forward-simulation without actually having a simulator. This is a really cool approach, basically trying to approximate a simulator specifically for the learning agent’s performance, not for actually simulating the task. We don’t care about simulating future board states, we only care about simulating future state values, rewards, or actions.\nWhat do they actually do? The authors use Monte Carlo Tree Search (MCTS) to choose future actions based on their learned forward simulator, the same way that AlphaZero chose actions based on the real simulator. Using MCTS to generate hidden-state roll-outs of the task (rather than full-on simulations), MuZero then learns 3 separate functions: the policy estimator, the value estimator, and the reward estimator. Each of these 3 functions uses the same hidden state, providing the signal for MuZero to learn a useful simulation function.\nThe policy estimator predicts which action should be taken for a given simulator step, the value function predicts the discounted reward (or value) back to a given simulator step, and the reward function predicts the reward value for a simulator step. Taken together, these three estimators provide all of the information that the MCTS actually needs to forward-simulate a game. The agent doesn’t need to know what the state actually looks like, it just needs to know which action it should take and what the estimated value and reward of the states are.\nDoes it work? MuZero works impressively well, as shown in the figure below (taken from the paper). The orange line represents AlphaZero’s performance for Go, Chess, and Shogi, and we can see that MuZero learns to replicate (and even exceed) AlphaZero’s performance in these domains (AlphaZero has access to a perfect simulator). In the Atari domain, the orange line is the prior state of the art, and again MuZero is able to exceed the baseline.\nOrange represents AlphaZero for Go, Chess, and Shogi and it shows the prior state of the art for Atari. In all cases, MuZero meets or exceeds the baselines, showing that MCTS with a learned, agent-specific simulator is a viable and successful approach to learning high-performing policies without having access to simulators. Image taken from https://arxiv.org/abs/1911.08265\nWhat can researchers take away from this paper? 1) You don’t need perfect simulators. MuZero is a strong example for not needing perfect simulators and for not recreating input states. It’s common to use some type of auto-encoder to pre-train a vision system in RL, or as an auxiliary loss to encourage the learning of useful embeddings. The authors in this work emphasize that pixel-perfect recreation isn’t useful to the agent, and it isn’t what we really care about. What we want is a useful policy and accurate estimates of future reward, not accurate estimates of future input states. So why not just try to optimize our hidden states and embeddings for the tasks we care about?\n2) Combine planning with deep-RL Both AlphaZero and now MuZero make a strong case for combining MCTS with more conventional deep-RL. Simply estimating actions and values is useful, but combining that with a look-ahead for longer-term planning is much more useful. Further, it seems necessary at the moment, as our agents don’t necessarily learn foresight or implicitly learn to plan on their own.\nClosing Thoughts \u0026 Discussion AlphaZero generated an incredible amount of buzz, but without access to staggering compute and accurate simulators, replicating AlphaZero hasn’t exactly been practically viable for many researchers.\nMuZero addresses one of these issues, but not the other. While we can now think about extending MCTS and deep-RL to a wider variety of domains, the actual process of MCTS + deep-RL still takes a seriously impressive amount of compute power and time, something that most academia researchers won’t realistically be able to replicate on the scale that a Google DeepMind can.\nI’m very curious to see some of the directions that DeepMind can take this technology. We’ve seen some of the lines of research like AlphaFold or Imitating Interactive Intelligence attack other interesting problems, problems which might benefit from learning the dynamics of the world and incorporating forward-simulation for planning. In particular, it would be very interesting to use human-expert trajectories to learn the hidden-state simulator, potentially reducing the compute needed for MuZero.\nFinally, I’m curious about the possibility of extracting a human-usable planner from MuZero. One of the reasons the algorithm works so well is that it doesn’t need to learn human-interpretable state dynamics, but this is also a limitation for many domains. If we could apply MuZero to learn a simulator of disease progression, we’d love to be able to open up that simulator and better-understand the dynamics for ourselves. Such pushes for human-interpretable simulators could be very useful in many domains.\nReferences and other useful resources: Abramson, Josh, et al. “Imitating Interactive Intelligence.” arXiv preprint arXiv:2012.05672 (2020).\nSilver, D., Huang, A., Maddison, C. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489 (2016). https://doi.org/10.1038/nature16961\nSilver, David, et al. “A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.” Science 362.6419 (2018): 1140-1144.\nSchrittwieser, J., Antonoglou, I., Hubert, T. et al. Mastering Atari, Go, chess and shogi by planning with a learned model. Nature 588, 604–609 (2020). https://doi.org/10.1038/s41586-020-03051-4\nYannic Kilcher’s video on the paper\nJulian Schrittwieser’s invited talk on the paper\n","wordCount":"1057","inLanguage":"en","datePublished":"2020-12-23T21:36:52-04:00","dateModified":"2020-12-23T21:36:52-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://andrew-silva.github.io/posts/muzero/"},"publisher":{"@type":"Organization","name":"Andrew Writing Things","logo":{"@type":"ImageObject","url":"https://andrew-silva.github.io/favicon.ico"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://andrew-silva.github.io/ accesskey=h title="Andrew Writing Things (Alt + H)">Andrew Writing Things</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://andrew-silva.github.io/posts/ title=posts><span>posts</span></a></li><li><a href=https://andrew-silva.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://andrew-silva.github.io/search title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://andrew-silva.com title=andrew-silva.com><span>andrew-silva.com</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Reading: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero Paper)</h1><div class=post-meta><span title='2020-12-23 21:36:52 -0400 -0400'>December 23, 2020</span></div></header><div class=post-content><p>DeepMind’s <a href=https://arxiv.org/abs/1911.08265>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</a> was officially <a href=https://www.nature.com/articles/s41586-020-03051-4>published today in Nature</a>, and is a reasonably short read covering a cool direction for future reinforcement learning (RL) research.</p><h2 id=what-is-the-paper-trying-to-do>What is the paper trying to do?<a hidden class=anchor aria-hidden=true href=#what-is-the-paper-trying-to-do>#</a></h2><p>Following the incredible successes of <a href=https://deepmind.google/technologies/alphago/><em>AlphaGo</em></a> and <a href=https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/><em>AlphaZero</em></a>, this work introduces <em>MuZero</em>, which permits the extension of <em>AlphaZero</em>-like learning and performance to a new set of RL domains.</p><p><em>AlphaZero</em> learned to master Go and Chess through the use of a simulator— RL agents use forward-simulation from each state in order to accurately estimate which move is the most valuable. So, from any given board state, the <em>AlphaZero</em> agent simulates many moves into the future, helping it to decide which move will yield the best long-term reward. <strong>Crucially, this only works if you have an accurate simulator</strong>. We can’t forward-simulate hundreds of steps for a task without a simulator.</p><p><em>MuZero</em> takes a step toward removing this restriction. The agent here learns to predict environment dynamics as they relate to the agent’s success, permitting forward-simulation without actually having a simulator. This is a really cool approach, basically trying to approximate a simulator specifically for the learning agent’s performance, not for actually simulating the task. We don’t care about simulating future board states, we only care about simulating future state values, rewards, or actions.</p><h2 id=what-do-they-actually-do>What do they actually do?<a hidden class=anchor aria-hidden=true href=#what-do-they-actually-do>#</a></h2><p>The authors use Monte Carlo Tree Search (MCTS) to choose future actions based on their learned forward simulator, the same way that <em>AlphaZero</em> chose actions based on the real simulator. Using MCTS to generate hidden-state roll-outs of the task (rather than full-on simulations), <em>MuZero</em> then learns 3 separate functions: the policy estimator, the value estimator, and the reward estimator. Each of these 3 functions uses the same hidden state, providing the signal for <em>MuZero</em> to learn a useful simulation function.</p><p>The policy estimator predicts which action should be taken for a given simulator step, the value function predicts the discounted reward (or value) back to a given simulator step, and the reward function predicts the reward value for a simulator step. Taken together, <strong>these three estimators provide all of the information that the MCTS actually needs to forward-simulate a game</strong>. The agent doesn’t need to know what the state actually looks like, it just needs to know which action it should take and what the estimated value and reward of the states are.</p><h2 id=does-it-work>Does it work?<a hidden class=anchor aria-hidden=true href=#does-it-work>#</a></h2><p><em>MuZero</em> works impressively well, as shown in the figure below (taken from the paper). The orange line represents <em>AlphaZero</em>&rsquo;s performance for Go, Chess, and Shogi, and we can see that <em>MuZero</em> learns to replicate (and even exceed) <em>AlphaZero</em>’s performance in these domains (<em>AlphaZero</em> has access to a perfect simulator). In the Atari domain, the orange line is the prior state of the art, and again <em>MuZero</em> is able to exceed the baseline.</p><p><img loading=lazy src=/muzero_performance.png alt="MuZero Results Plots">
<em>Orange represents <em>AlphaZero</em> for Go, Chess, and Shogi and it shows the prior state of the art for Atari. In all cases, <em>MuZero</em> meets or exceeds the baselines, showing that MCTS with a learned, agent-specific simulator is a viable and successful approach to learning high-performing policies without having access to simulators. Image taken from <a href=https://arxiv.org/abs/1911.08265>https://arxiv.org/abs/1911.08265</a></em></p><h2 id=what-can-researchers-take-away-from-this-paper>What can researchers take away from this paper?<a hidden class=anchor aria-hidden=true href=#what-can-researchers-take-away-from-this-paper>#</a></h2><h3 id=1-you-dont-need-perfect-simulators>1) You don’t need perfect simulators.<a hidden class=anchor aria-hidden=true href=#1-you-dont-need-perfect-simulators>#</a></h3><p><em>MuZero</em> is a strong example for not needing perfect simulators and for not recreating input states. It’s common to use some type of auto-encoder to pre-train a vision system in RL, or as an auxiliary loss to encourage the learning of useful embeddings. The authors in this work emphasize that pixel-perfect recreation isn’t useful to the agent, and it isn’t what we really care about. What we want is a useful policy and accurate estimates of future reward, not accurate estimates of future input states. So why not just try to optimize our hidden states and embeddings for the tasks we care about?</p><h3 id=2-combine-planning-with-deep-rl>2) Combine planning with deep-RL<a hidden class=anchor aria-hidden=true href=#2-combine-planning-with-deep-rl>#</a></h3><p>Both <em>AlphaZero</em> and now <em>MuZero</em> make a strong case for combining MCTS with more conventional deep-RL. Simply estimating actions and values is useful, but combining that with a look-ahead for longer-term planning is much more useful. Further, it seems necessary at the moment, as our agents don’t necessarily learn foresight or implicitly learn to plan on their own.</p><h2 id=closing-thoughts--discussion>Closing Thoughts & Discussion<a hidden class=anchor aria-hidden=true href=#closing-thoughts--discussion>#</a></h2><p><em>AlphaZero</em> generated an incredible amount of buzz, but without access to staggering compute and accurate simulators, replicating <em>AlphaZero</em> hasn’t exactly been practically viable for many researchers.</p><p><em>MuZero</em> addresses one of these issues, but not the other. While we can now think about extending MCTS and deep-RL to a wider variety of domains, the actual process of MCTS + deep-RL still takes a seriously impressive amount of compute power and time, something that most academia researchers won’t realistically be able to replicate on the scale that a Google DeepMind can.</p><p>I’m very curious to see some of the directions that DeepMind can take this technology. We’ve seen some of the lines of research like AlphaFold or Imitating Interactive Intelligence attack other interesting problems, problems which might benefit from learning the dynamics of the world and incorporating forward-simulation for planning. In particular, it would be very interesting to use human-expert trajectories to learn the hidden-state simulator, potentially reducing the compute needed for <em>MuZero</em>.</p><p>Finally, I’m curious about the possibility of extracting a human-usable planner from <em>MuZero</em>. One of the reasons the algorithm works so well is that it doesn’t need to learn human-interpretable state dynamics, but this is also a limitation for many domains. If we could apply <em>MuZero</em> to learn a simulator of disease progression, we’d love to be able to open up that simulator and better-understand the dynamics for ourselves. Such pushes for human-interpretable simulators could be very useful in many domains.</p><h2 id=references-and-other-useful-resources>References and other useful resources:<a hidden class=anchor aria-hidden=true href=#references-and-other-useful-resources>#</a></h2><p>Abramson, Josh, et al. &ldquo;Imitating Interactive Intelligence.&rdquo; arXiv preprint arXiv:2012.05672 (2020).</p><p>Silver, D., Huang, A., Maddison, C. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489 (2016). <a href=https://doi.org/10.1038/nature16961>https://doi.org/10.1038/nature16961</a></p><p>Silver, David, et al. &ldquo;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&rdquo; Science 362.6419 (2018): 1140-1144.</p><p>Schrittwieser, J., Antonoglou, I., Hubert, T. et al. Mastering Atari, Go, chess and shogi by planning with a learned model. Nature 588, 604–609 (2020). <a href=https://doi.org/10.1038/s41586-020-03051-4>https://doi.org/10.1038/s41586-020-03051-4</a></p><p><a href="https://www.youtube.com/watch?ab_channel=YannicKilcher&amp;v=We20YSAJZSE">Yannic Kilcher’s video on the paper</a></p><p><a href="https://www.youtube.com/watch?v=L0A86LmH7Yw">Julian Schrittwieser’s invited talk on the paper</a></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://andrew-silva.github.io/>Andrew Writing Things</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>