<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Interactive Intelligence | Andrew Writing Things</title>
<meta name=keywords content><meta name=description content="Google DeepMind recently put a new 96 page paper on arXiv titled Imitating Interactive Intelligence. The paper actually has a few really cool things to contribute. Unfortunately, owing to its very intimidating length, it’s not easy to actually gain these insights. DeepMind have actually put up their own blog post on the work, but I didn’t really get a sense for what happened just by reading that post. They also released three videos showing what happened in the project (overview, training timelapse, and a demo)."><meta name=author content><link rel=canonical href=https://andrew-silva.github.io/posts/interactive_intelligence/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://andrew-silva.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://andrew-silva.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://andrew-silva.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://andrew-silva.github.io/apple-touch-icon.png><link rel=mask-icon href=https://andrew-silva.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B1B9JZ8WY8"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B1B9JZ8WY8",{anonymize_ip:!1})}</script><meta property="og:title" content="Interactive Intelligence"><meta property="og:description" content="Google DeepMind recently put a new 96 page paper on arXiv titled Imitating Interactive Intelligence. The paper actually has a few really cool things to contribute. Unfortunately, owing to its very intimidating length, it’s not easy to actually gain these insights. DeepMind have actually put up their own blog post on the work, but I didn’t really get a sense for what happened just by reading that post. They also released three videos showing what happened in the project (overview, training timelapse, and a demo)."><meta property="og:type" content="article"><meta property="og:url" content="https://andrew-silva.github.io/posts/interactive_intelligence/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-20T21:36:45-04:00"><meta property="article:modified_time" content="2020-12-20T21:36:45-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Interactive Intelligence"><meta name=twitter:description content="Google DeepMind recently put a new 96 page paper on arXiv titled Imitating Interactive Intelligence. The paper actually has a few really cool things to contribute. Unfortunately, owing to its very intimidating length, it’s not easy to actually gain these insights. DeepMind have actually put up their own blog post on the work, but I didn’t really get a sense for what happened just by reading that post. They also released three videos showing what happened in the project (overview, training timelapse, and a demo)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://andrew-silva.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Interactive Intelligence","item":"https://andrew-silva.github.io/posts/interactive_intelligence/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Interactive Intelligence","name":"Interactive Intelligence","description":"Google DeepMind recently put a new 96 page paper on arXiv titled Imitating Interactive Intelligence. The paper actually has a few really cool things to contribute. Unfortunately, owing to its very intimidating length, it’s not easy to actually gain these insights. DeepMind have actually put up their own blog post on the work, but I didn’t really get a sense for what happened just by reading that post. They also released three videos showing what happened in the project (overview, training timelapse, and a demo).","keywords":[],"articleBody":"Google DeepMind recently put a new 96 page paper on arXiv titled Imitating Interactive Intelligence. The paper actually has a few really cool things to contribute. Unfortunately, owing to its very intimidating length, it’s not easy to actually gain these insights. DeepMind have actually put up their own blog post on the work, but I didn’t really get a sense for what happened just by reading that post. They also released three videos showing what happened in the project (overview, training timelapse, and a demo). This write-up will summarize:\nWhat the paper is trying to do, What it actually does, Whether or not their approach works or insights are accurate, and What machine learning/robotics researchers can take away from the paper. What is the paper trying to do? This paper is targeted at developing agents which have “interactive intelligence.” Effectively, if we have a robot in a child’s bedroom and we tell it to “Put the toy helicopter back on the shelf,” then can our robots understand that command and execute it? If we ask the robot “What color is the notebook on the desk?” then can our robots answer the question? If we want the robot to coordinate other agents in a cleanup task, can it generate sub-commands that conform to specific instances of “Tell people to put things away?”\nDeepMind extends their prior success in imitation learning (IL) and reinforcement learning (RL) to language learning, vision learning, and task learning in their new Playroom Unity environment (shown below), an interactive environment for simple task completion. After gathering roughly 2 years of data, the authors ask the question: Can we learn to understand and respond to language commands in the Playroom? Can we learn to be an interactive, intelligent agent in the Playroom by imitating 2 years of expert data?\nFigure 1 from the paper itself. Panel A shows a closeup of a Playroom, with two agents looking at a toy helicopter. Panel B shows four examples of the dynamic, random configurations of the Playroom. Panel C shows examples of various objects which could be randomly distributed in a Playroom. Image source: https://arxiv.org/pdf/2012.05672.pdf\nWhat do they do? The authors compare several training schemes, loss functions, model architectures, dataset sizes, and success metrics to see what works best for learning interactive intelligence. Ultimately, they introduce several interesting problems (how do we know when tasks are successfully assigned or completed without having humans label every episode?), empirically determine that it’s important to balance various objectives, and produce a tangible, end-to-end system that takes a concrete step towards imitating interactive intelligence from a large human dataset.\nImportantly, there is not an actual robot deployment (this is entirely in simulation), the entirety of the dataset/task is confined to the Playroom, and unfortunately there isn’t much shared with the community outside of the paper (no code, environment, or data).\nDoes it actually work? The authors’ approach does seem to do quite well! Owing to their thorough ablation and sweep over architectures, training procedures, dataset sizes, and more, we can be reasonably confident that their approach would work well for learning situated intelligence in Playroom-like environments (given their assumptions of constrained vocabularies, action spaces, etc).\nWhat can researchers take away from this paper? 1) Use Imitation Learning + Reinforcement Learning + Representation Learning Following DeepMind’s previous IL+RL success in AlphaStar and related recent works from others at CVPR, UAI, and more, Imitating Interactive Intelligence adds evidence for the success of IL + RL for everything from task-learning to language-grounding, generalization, and transfer learning. The authors here contribute an in-depth comparison of different techniques that can be combined to learn task this interactive intelligence. For interactive task completion, question-answering, and producing language commands, the authors conclude that the best way to learn is to combine behavior cloning with RL and a few auxiliary tasks for representation learning (such as “object in scene prediction” and “goal-trajectory matching”).\n2) Use Transformers for Multi-Modal Scene Embeddings This paper is also the latest showing just how good transformers are. The authors use ResNet to embed images into flat vectors, then pass image and word embeddings into a multi-modal transformer to obtain scene embeddings. These scene embeddings then go to an LSTM, where the hidden-states are used for the various policy heads (as shown in the figure from the paper below). Nothing here is particularly surprising or new, but the authors do a very exhaustive comparison of different architectures, and it’s worth noting that, once again, transformers are just so good at learning scene embeddings.\nFigure 5 in the original paper. Images are embedded using ResNet, words are tokenized into a 500-word vocabulary, and then these embeddings all go to a transformer. Transformer tokens go into an LSTM, which produces the hidden states for the various policy heads. Image source: https://arxiv.org/pdf/2012.05672.pdf\nThe authors do not share their dataset, their code, their environments, or their learned models. A tremendous amount of space in the paper is dedicated to explaining the problem setup, the data collection process, the annotation process, and very fine-grained details in the appendix. For anyone looking to get into reinforcement learning, there are a myriad of design decisions, little bugs to track down, and headaches to overcome. The appendix here covers all of it. It’s one of my favorite parts of the paper, just because it is a write-up of all of the things I usually want to document for my own research.\nClosing Thoughts \u0026 Discussion Last year, DeepMind released AlphaStar, an AI StarCraft II player that could compete at a professional level in a very complex real-time strategy game. While DeepMind had success in Go and Chess by just using reinforcement learning, it turns out that reinforcement learning isn’t enough for extremely complex tasks (like StarCraft II). Their StarCraft II AI learned by copying the strategies of millions of human players while also learning by exploration and self-play. This work extends that line of thinking, effectively trying to learn the very complex “game” of interactive intelligence by imitating and supervising over human interactions and applying reinforcement learning. What’s particularly cool here is that the reinforcement learning problem for interactive intelligence isn’t very well-defined, so they apply GAIL to estimate a dynamic reward function for their agent.\nThis work was one of the first I’ve seen to directly approach NLP from a situated, interactive standpoint. As highlighted in prior work, trying to learn language just by reading the internet is a weird and probably bad idea. Learning language by interacting with the world, copying other people’s behavior, and completing tasks makes much more sense and is more “how we learn.” I really like this direction, and I’m excited to see more research with this motivation.\nFinally, I want to re-iterate how great the effort at reproducibility is here. While we don’t have code or data, the authors put a tremendous amount of work into documenting everything they tested, all of the hyperparameter choices they made, all of their experiments, etc. It’s great to see, and it will be a great resource for beginner RL researchers.\nWorks linked in this post and other cool work in the area: Abramson, Josh, et al. “Imitating Interactive Intelligence.” arXiv preprint arXiv:2012.05672 (2020).\nBisk, Yonatan, et al. “Experience grounds language.” arXiv preprint arXiv:2004.10151 (2020).\nChing-An Cheng, Xinyan Yan, Nolan Wagener, Byron Boots “Fast Policy Learning through Imitation and Reinforcement.” UAI 2018: 845-855.\nHo, Jonathan, and Stefano Ermon. “Generative adversarial imitation learning.” Advances in neural information processing systems 29 (2016): 4565-4573.\nSilver, David, et al. “Mastering the game of Go with deep neural networks and tree search.” nature 529.7587 (2016): 484-489.\nTeam, AlphaStar. “Alphastar: Mastering the real-time strategy game starcraft ii.” DeepMind blog 24 (2019) https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii\nWang, Xin, et al. “Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\nWard, Tom, et al. “Using Unity to Help Solve Intelligence.” arXiv preprint arXiv:2011.09294 (2020).\nWu, Ga, et al. “Deep language-based critiquing for recommender systems.” Proceedings of the 13th ACM Conference on Recommender Systems. 2019.\n","wordCount":"1342","inLanguage":"en","datePublished":"2020-12-20T21:36:45-04:00","dateModified":"2020-12-20T21:36:45-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://andrew-silva.github.io/posts/interactive_intelligence/"},"publisher":{"@type":"Organization","name":"Andrew Writing Things","logo":{"@type":"ImageObject","url":"https://andrew-silva.github.io/favicon.ico"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://andrew-silva.github.io/ accesskey=h title="Andrew Writing Things (Alt + H)">Andrew Writing Things</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://andrew-silva.github.io/posts/ title=posts><span>posts</span></a></li><li><a href=https://andrew-silva.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://andrew-silva.github.io/search title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://andrew-silva.com title=andrew-silva.com><span>andrew-silva.com</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Interactive Intelligence</h1><div class=post-meta><span title='2020-12-20 21:36:45 -0400 -0400'>December 20, 2020</span></div></header><div class=post-content><p><a href=https://deepmind.google>Google DeepMind</a> recently put a new <strong>96 page</strong> paper on arXiv titled <a href=https://arxiv.org/abs/2012.05672>Imitating Interactive Intelligence</a>. The paper actually has a few really cool things to contribute. Unfortunately, owing to its very intimidating length, it’s not easy to actually gain these insights. DeepMind have actually put up <a href=https://deepmind.google/discover/blog/imitating-interactive-intelligence/>their own blog post on the work</a>, but I didn’t really get a sense for what happened just by reading that post. They also released three videos showing what happened in the project (<a href="https://www.youtube.com/watch?ab_channel=GregoryWayne&amp;feature=youtu.be&amp;v=b-fvsi9YIP4">overview</a>, <a href="https://www.youtube.com/watch?v=NwzRR7XD898">training timelapse</a>, and a <a href="https://www.youtube.com/watch?v=510xBEcef_o">demo</a>). This write-up will summarize:</p><ol><li>What the paper is trying to do,</li><li>What it actually does,</li><li>Whether or not their approach works or insights are accurate, and</li><li>What machine learning/robotics researchers can take away from the paper.</li></ol><h2 id=what-is-the-paper-trying-to-do>What is the paper trying to do?<a hidden class=anchor aria-hidden=true href=#what-is-the-paper-trying-to-do>#</a></h2><p>This paper is targeted at developing agents which have &ldquo;<strong>interactive intelligence</strong>.” Effectively, if we have a robot in a child&rsquo;s bedroom and we tell it to “Put the toy helicopter back on the shelf,” then <strong>can our robots understand that command and execute it</strong>? If we ask the robot “What color is the notebook on the desk?” then <strong>can our robots answer the question</strong>? If we want the robot to coordinate other agents in a cleanup task, <strong>can it generate sub-commands</strong> that conform to specific instances of “Tell people to put things away?”</p><p>DeepMind extends their prior success in imitation learning (IL) and reinforcement learning (RL) to language learning, vision learning, and task learning in their new <a href=https://deepmind.google/discover/blog/using-unity-to-help-solve-intelligence/>Playroom Unity environment</a> (shown below), an interactive environment for simple task completion. After gathering roughly 2 years of data, the authors ask the question: Can we learn to understand and respond to language commands in the Playroom? Can we learn to be an interactive, intelligent agent in the Playroom by imitating 2 years of expert data?</p><p><img loading=lazy src=/imitating-interactive-intelligence_playroom.png alt="Figure 1 from Imitating Interactive Intelligence">
<em>Figure 1 from the paper itself. Panel A shows a closeup of a Playroom, with two agents looking at a toy helicopter. Panel B shows four examples of the dynamic, random configurations of the Playroom. Panel C shows examples of various objects which could be randomly distributed in a Playroom. Image source: <a href=https://arxiv.org/pdf/2012.05672.pdf>https://arxiv.org/pdf/2012.05672.pdf</a></em></p><h2 id=what-do-they-do>What do they do?<a hidden class=anchor aria-hidden=true href=#what-do-they-do>#</a></h2><p>The authors compare several training schemes, loss functions, model architectures, dataset sizes, and success metrics to see what works best for learning interactive intelligence. Ultimately, <strong>they introduce several interesting problems</strong> (how do we know when tasks are successfully assigned or completed without having humans label every episode?), <strong>empirically determine that it’s important to balance various objectives</strong>, and produce a tangible, end-to-end system that <strong>takes a concrete step towards imitating interactive intelligence</strong> from a large human dataset.</p><p>Importantly, there is not an actual robot deployment (this is entirely in simulation), the entirety of the dataset/task is confined to the Playroom, and unfortunately there isn’t much shared with the community outside of the paper (no code, environment, or data).</p><h2 id=does-it-actually-work>Does it actually work?<a hidden class=anchor aria-hidden=true href=#does-it-actually-work>#</a></h2><p><strong>The authors’ approach does seem to do quite well!</strong> Owing to their thorough ablation and sweep over architectures, training procedures, dataset sizes, and more, we can be reasonably confident that their approach would work well for learning situated intelligence in Playroom-like environments (given their assumptions of constrained vocabularies, action spaces, etc).</p><h2 id=what-can-researchers-take-away-from-this-paper>What can researchers take away from this paper?<a hidden class=anchor aria-hidden=true href=#what-can-researchers-take-away-from-this-paper>#</a></h2><h3 id=1-use-imitation-learning--reinforcement-learning--representation-learning>1) Use Imitation Learning + Reinforcement Learning + Representation Learning<a hidden class=anchor aria-hidden=true href=#1-use-imitation-learning--reinforcement-learning--representation-learning>#</a></h3><p>Following DeepMind’s previous IL+RL success in <a href=https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/>AlphaStar</a> and related recent works from others at <a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Reinforced_Cross-Modal_Matching_and_Self-Supervised_Imitation_Learning_for_Vision-Language_Navigation_CVPR_2019_paper.pdf>CVPR</a>, <a href=https://arxiv.org/abs/1805.10413>UAI</a>, and <a href=https://dl.acm.org/doi/abs/10.1145/3298689.3347009>more</a>, <strong><em>Imitating Interactive Intelligence</em> adds evidence for the success of IL + RL</strong> for everything from task-learning to language-grounding, generalization, and transfer learning. The authors here contribute an in-depth comparison of different techniques that can be combined to learn task this interactive intelligence. For interactive task completion, question-answering, and producing language commands, the authors conclude that the <strong>best way to learn is to combine behavior cloning with RL and a few auxiliary tasks for representation learning</strong> (such as “object in scene prediction” and “goal-trajectory matching”).</p><h3 id=2-use-transformers-for-multi-modal-scene-embeddings>2) Use Transformers for Multi-Modal Scene Embeddings<a hidden class=anchor aria-hidden=true href=#2-use-transformers-for-multi-modal-scene-embeddings>#</a></h3><p>This paper is also the latest showing just how good transformers are. The authors use ResNet to embed images into flat vectors, then pass image and word embeddings into a multi-modal transformer to obtain scene embeddings. These scene embeddings then go to an LSTM, where the hidden-states are used for the various policy heads (as shown in the figure from the paper below). Nothing here is particularly surprising or new, but the authors do a very exhaustive comparison of different architectures, and it’s worth noting that, once again, <strong>transformers are just so good at learning scene embeddings.</strong></p><p><img loading=lazy src=/imitating-interactive-intelligence_architecture.png alt="Results from Imitating Interactive Intelligence">
<em>Figure 5 in the original paper. Images are embedded using ResNet, words are tokenized into a 500-word vocabulary, and then these embeddings all go to a transformer. Transformer tokens go into an LSTM, which produces the hidden states for the various policy heads. Image source: <a href=https://arxiv.org/pdf/2012.05672.pdf>https://arxiv.org/pdf/2012.05672.pdf</a></em></p><p>The authors do not share their dataset, their code, their environments, or their learned models. A tremendous amount of space in the paper is dedicated to explaining the problem setup, the data collection process, the annotation process, and very fine-grained details in the appendix. For anyone looking to get into reinforcement learning, there are a myriad of design decisions, little bugs to track down, and headaches to overcome. <strong>The appendix here covers all of it. It’s one of my favorite parts of the paper</strong>, just because it is a write-up of all of the things I usually want to document for my own research.</p><h2 id=closing-thoughts--discussion>Closing Thoughts & Discussion<a hidden class=anchor aria-hidden=true href=#closing-thoughts--discussion>#</a></h2><p>Last year, DeepMind released <a href=https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/>AlphaStar</a>, an AI StarCraft II player that could compete at a professional level in a very complex real-time strategy game. While DeepMind had success in Go and Chess by just using reinforcement learning, it turns out that reinforcement learning isn’t enough for extremely complex tasks (like StarCraft II). Their StarCraft II AI learned by copying the strategies of millions of human players while also learning by exploration and self-play. This work extends that line of thinking, effectively trying to learn the very complex “game” of interactive intelligence by imitating and supervising over human interactions and applying reinforcement learning. <strong>What’s particularly cool here</strong> is that the reinforcement learning problem for interactive intelligence isn’t very well-defined, so they apply <a href=https://arxiv.org/abs/1606.03476>GAIL</a> to estimate a dynamic reward function for their agent.</p><p>This work was <strong>one of the first I’ve seen to directly approach NLP from a situated, interactive standpoint</strong>. As highlighted in <a href=https://arxiv.org/abs/2004.10151>prior work</a>, trying to learn language just by reading the internet is a weird and probably bad idea. Learning language by interacting with the world, copying other people’s behavior, and completing tasks makes much more sense and is more “how we learn.” I really like this direction, and I’m excited to see more research with this motivation.</p><p>Finally, <strong>I want to re-iterate how great the effort at reproducibility is here.</strong> While we don’t have code or data, the authors put a tremendous amount of work into documenting everything they tested, all of the hyperparameter choices they made, all of their experiments, etc. It’s great to see, and it will be a great resource for beginner RL researchers.</p><h2 id=works-linked-in-this-post-and-other-cool-work-in-the-area>Works linked in this post and other cool work in the area:<a hidden class=anchor aria-hidden=true href=#works-linked-in-this-post-and-other-cool-work-in-the-area>#</a></h2><p>Abramson, Josh, et al. &ldquo;Imitating Interactive Intelligence.&rdquo; arXiv preprint arXiv:2012.05672 (2020).</p><p>Bisk, Yonatan, et al. &ldquo;Experience grounds language.&rdquo; arXiv preprint arXiv:2004.10151 (2020).</p><p>Ching-An Cheng, Xinyan Yan, Nolan Wagener, Byron Boots “Fast Policy Learning through Imitation and Reinforcement.” UAI 2018: 845-855.</p><p>Ho, Jonathan, and Stefano Ermon. &ldquo;Generative adversarial imitation learning.&rdquo; Advances in neural information processing systems 29 (2016): 4565-4573.</p><p>Silver, David, et al. &ldquo;Mastering the game of Go with deep neural networks and tree search.&rdquo; nature 529.7587 (2016): 484-489.</p><p>Team, AlphaStar. &ldquo;Alphastar: Mastering the real-time strategy game starcraft ii.&rdquo; DeepMind blog 24 (2019) <a href=https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii>https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii</a></p><p>Wang, Xin, et al. &ldquo;Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation.&rdquo; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p><p>Ward, Tom, et al. &ldquo;Using Unity to Help Solve Intelligence.&rdquo; arXiv preprint arXiv:2011.09294 (2020).</p><p>Wu, Ga, et al. &ldquo;Deep language-based critiquing for recommender systems.&rdquo; Proceedings of the 13th ACM Conference on Recommender Systems. 2019.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://andrew-silva.github.io/>Andrew Writing Things</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>