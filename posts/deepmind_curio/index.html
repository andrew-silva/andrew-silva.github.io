<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reading: Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward | Andrew Writing Things</title>
<meta name=keywords content><meta name=description content="I recently read Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward and found it an interesting read, so I figured I&rsquo;d write a quick summary and some thoughts on the work.
Motivation We want to personalize LLMs in situ and without assuming access to labeled preference datasets for users. If we treat the user as a world that the LLM must explore, then a curiosity or intrinsic exploration reward should help us to discover the user&rsquo;s innate preferences more quickly and effectively."><meta name=author content><link rel=canonical href=https://andrew-silva.github.io/posts/deepmind_curio/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://andrew-silva.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://andrew-silva.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://andrew-silva.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://andrew-silva.github.io/apple-touch-icon.png><link rel=mask-icon href=https://andrew-silva.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B1B9JZ8WY8"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B1B9JZ8WY8",{anonymize_ip:!1})}</script><meta property="og:title" content="Reading: Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward"><meta property="og:description" content="I recently read Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward and found it an interesting read, so I figured I&rsquo;d write a quick summary and some thoughts on the work.
Motivation We want to personalize LLMs in situ and without assuming access to labeled preference datasets for users. If we treat the user as a world that the LLM must explore, then a curiosity or intrinsic exploration reward should help us to discover the user&rsquo;s innate preferences more quickly and effectively."><meta property="og:type" content="article"><meta property="og:url" content="https://andrew-silva.github.io/posts/deepmind_curio/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-11T16:06:02-04:00"><meta property="article:modified_time" content="2025-07-11T16:06:02-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reading: Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward"><meta name=twitter:description content="I recently read Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward and found it an interesting read, so I figured I&rsquo;d write a quick summary and some thoughts on the work.
Motivation We want to personalize LLMs in situ and without assuming access to labeled preference datasets for users. If we treat the user as a world that the LLM must explore, then a curiosity or intrinsic exploration reward should help us to discover the user&rsquo;s innate preferences more quickly and effectively."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://andrew-silva.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Reading: Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward","item":"https://andrew-silva.github.io/posts/deepmind_curio/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reading: Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward","name":"Reading: Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward","description":"I recently read Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward and found it an interesting read, so I figured I\u0026rsquo;d write a quick summary and some thoughts on the work.\nMotivation We want to personalize LLMs in situ and without assuming access to labeled preference datasets for users. If we treat the user as a world that the LLM must explore, then a curiosity or intrinsic exploration reward should help us to discover the user\u0026rsquo;s innate preferences more quickly and effectively.","keywords":[],"articleBody":"I recently read Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward and found it an interesting read, so I figured I’d write a quick summary and some thoughts on the work.\nMotivation We want to personalize LLMs in situ and without assuming access to labeled preference datasets for users. If we treat the user as a world that the LLM must explore, then a curiosity or intrinsic exploration reward should help us to discover the user’s innate preferences more quickly and effectively.\nMethod In this work, the authors use an LLM as a synthetic human (the “Environment Model”, because the user is likened to the environment in conventional reinforcement learning). The core of the method then relies on 3 other LLMs. One converses with the “human” (this is the LLM we’re learning/updating, called the “Policy Model”). The next scores the finished conversation and generates the task reward (the “Reward Model”). And finally, one LLM (the “User Model”) predicts a dense, turn-based reward based on how well the conversation (between the Environment Model and Policy Model) enables the User Model to infer the synthetic user’s preferences. The Environment Model, Policy Model, and Reward Model are all Gemma 2B checkpoints, and the User Model is a Gemma 7B checkpoint.\nThe full pipeline, called CURIO ((Curiosity-driven User-modeling Reward as Intrinsic Objective) This inference of a user’s preferences is the key part of their method, as this determines the dense, intrinsic reward that they claim can improve personalization when added to sparse, end-of-conversation rewards. These intrinsic rewards are derived using the User Modeling LLM’s logits over the user’s type (which suggests we need to know a user’s ground truth “type” before training). And we can use those likelihoods as either:\nbase likelihoods (the Policy Model should ask questions that maximize the User Model’s inference of the user’s true type) log likelihoods (as above, but with log likelihoods) negative entropy (the Policy Model should be reducing the User Model’s entropy about the user’s type – note that this does not require us to know the user’s ground truth preferences, but also does not guarantee that we’re increasing likelihood of the correct preferences.) Finally, the authors frame these objectives as potential-based rewards, meaning that the objective is modified such that we optimize for improvements in each metric in each conversation turn, rather than simply trying to maximize the objective over the conversation.\nFindings The authors find that their three auxiliary personalization objectives, particularly when framed as potential-based rewards, lead to significant improvements in personalization performance. Potential-based accuracy rewards (maximizing the base likelihoods) lead to a significant performance increase (from just ~68% up to ~87% success rates in suggesting personalized exercises), which makes sense given that this method assumes access to ground-truth information about the users. However, encouragingly, the potential-based negative entropy reward also leads to large improvements (up to ~84%), meaning that such improvements may not necessarily require prior knowledge about the user’s ground truth preferences. Unfortunately, this wasn’t a universal trend and the entropy-based methods ended up doing much worse on other evals (such as conversation quality and personalization for a more subtle task).\n(Left) CURIO conversations a much better than baseline model conversations at helping a User Model to predict user preferences. (Right) Success rates for personalized LLMs trained via CURIO with potential-based accuracy (DiffAcc) vs baseline RL. Takeaways Reinforcement learning can effectively improve personalization beyond SFT on personalization data. While this isn’t news, it is good to see this borne out in the data and on the authors’ LLM-simulator setup.\nIntrinsic rewards improve learning efficiency and efficacy when applying RL for personalization with LLMs. Particularly when there is ground-truth information about users, applying auxiliary objectives that encourage the LLM to produce conversations that reveal user preferences can lead to improved personalization.\nNaively improving personalization performance can lead to drops in conversation quality. The authors find that several versions of their intrinsic rewards lead to reward-hacking behaviors such as arbitrary lengthening of responses, mode collapse to a single preference type, or “persuasion” behaviors (where the LLM tries to change the user’s preference rather than infer it). Looking through the appendix, it seems like the Environment Model (the LLM playing a synthetic human) tends to be very… LLM-y in its responses, leading to poor personalization performance and bad overall conversations, as the synthetic user opts to agree with suggestions and mindlessly agrees with everything that the Policy Model suggests, rather than behaving more like a real user.\nThoughts \u0026 Future work I like the idea of adding some auxiliary objective to the LLM that focuses on the conversation as a whole, rather than just on the end-product. While the motivation of learning without labeled datasets didn’t entirely match the methods (which assumed access to labeled human preferences), I think the findings and lessons are valuable for future research, and there are plenty of avenues opened up by this work.\nFirst, it would be interesting to explore how we might relax the assumptions that we must know a user’s preferences. Future work might explore more ideas like the entropy-based rewards in this work, where the LLM is encouraged to “find out more”, even if it doesn’t know exactly what it’s looking for.\nSeparately, it may be interesting to explore adding other objectives that improve the conversation quality or that help to mitigate reward hacking. For example, we could add dense rewards for conversations that are particularly informative, engaging, or thought-provoking, in addition to those that help reveal hidden preferences.\nFinally, I appreciate the investment in modeling personalization as a multi-turn problem, in which the LLM has a chance to “talk” to a user before making a suggestion. Even so, this work assumes that users are static, and that preferences do not change with changing context (location, time-of-day, mood, etc.) or evolve over time. I’m interested in seeing future work that considers how preferences evolve over time or in response to changing settings.\nReferences [1] Wan, Yanming, Jiaxing Wu, Marwa Abdulhai, Lior Shani, and Natasha Jaques. “Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward.” arXiv preprint arXiv:2504.03206 (2025). https://arxiv.org/pdf/2504.03206\n","wordCount":"1010","inLanguage":"en","datePublished":"2025-07-11T16:06:02-04:00","dateModified":"2025-07-11T16:06:02-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://andrew-silva.github.io/posts/deepmind_curio/"},"publisher":{"@type":"Organization","name":"Andrew Writing Things","logo":{"@type":"ImageObject","url":"https://andrew-silva.github.io/favicon.ico"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://andrew-silva.github.io/ accesskey=h title="Andrew Writing Things (Alt + H)">Andrew Writing Things</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://andrew-silva.github.io/posts/ title=posts><span>posts</span></a></li><li><a href=https://andrew-silva.github.io/search title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://andrew-silva.com title=andrew-silva.com><span>andrew-silva.com</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Reading: Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</h1><div class=post-meta><span title='2025-07-11 16:06:02 -0400 EDT'>July 11, 2025</span></div></header><div class=post-content><p>I recently read <a href=https://arxiv.org/pdf/2504.03206v2>Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</a> and found it an interesting read, so I figured I&rsquo;d write a quick summary and some thoughts on the work.</p><h3 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h3><p>We want to personalize LLMs <em>in situ</em> and without assuming access to labeled preference datasets for users.
If we treat the user as a world that the LLM must explore, then a curiosity or intrinsic exploration reward should help us to discover the user&rsquo;s innate preferences more quickly and effectively.</p><h3 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h3><p>In this work, the authors use an LLM as a synthetic human (the &ldquo;Environment Model&rdquo;, because the user is likened to the environment in conventional reinforcement learning).
The core of the method then relies on 3 <em>other</em> LLMs. One converses with the &ldquo;human&rdquo; (this is the LLM we&rsquo;re learning/updating, called the &ldquo;Policy Model&rdquo;). The next scores the finished conversation and generates the task reward (the &ldquo;Reward Model&rdquo;).
And finally, one LLM (the &ldquo;User Model&rdquo;) predicts a dense, turn-based reward based on how well the conversation (between the Environment Model and Policy Model) enables the User Model to infer the synthetic user&rsquo;s preferences.
The Environment Model, Policy Model, and Reward Model are all Gemma 2B checkpoints, and the User Model is a Gemma 7B checkpoint.</p><figure><img src=/CURIO_overview.png alt="CURIO overview diagram"><figcaption>The full pipeline, called CURIO ((Curiosity-driven User-modeling Reward as Intrinsic Objective)</figcaption></figure><p>This inference of a user&rsquo;s preferences is the key part of their method, as this determines the dense, intrinsic reward that they claim can improve personalization when added to sparse, end-of-conversation rewards.
These intrinsic rewards are derived using the User Modeling LLM&rsquo;s logits over the user&rsquo;s type (which suggests we need to know a user&rsquo;s ground truth &ldquo;type&rdquo; before training).
And we can use those likelihoods as either:</p><ol><li><strong>base likelihoods</strong> (the Policy Model should ask questions that maximize the User Model&rsquo;s inference of the user&rsquo;s true type)</li><li><strong>log likelihoods</strong> (as above, but with log likelihoods)</li><li><strong>negative entropy</strong> (the Policy Model should be reducing the User Model&rsquo;s entropy about the user&rsquo;s type &ndash; note that this does <em>not</em> require us to know the user&rsquo;s ground truth preferences, but also does not guarantee that we&rsquo;re increasing likelihood of the <em>correct</em> preferences.)</li></ol><p>Finally, the authors frame these objectives as <em>potential-based rewards</em>, meaning that the objective is modified such that we optimize for improvements in each metric in each conversation turn, rather than simply trying to maximize the objective over the conversation.</p><h3 id=findings>Findings<a hidden class=anchor aria-hidden=true href=#findings>#</a></h3><p>The authors find that their three auxiliary personalization objectives, particularly when framed as potential-based rewards, lead to significant improvements in personalization performance.
Potential-based accuracy rewards (maximizing the base likelihoods) lead to a significant performance increase (from just ~68% up to ~87% success rates in suggesting personalized exercises), which makes sense given that this method assumes access to ground-truth information about the users.
However, encouragingly, the potential-based negative entropy reward <em>also</em> leads to large improvements (up to ~84%), meaning that such improvements may not necessarily require prior knowledge about the user&rsquo;s ground truth preferences.
Unfortunately, this wasn&rsquo;t a universal trend and the entropy-based methods ended up doing much worse on other evals (such as conversation quality and personalization for a more subtle task).</p><figure><img src=/curio_plots.png alt="CURIO results plots"><figcaption>(Left) CURIO conversations a much better than baseline model conversations at helping a User Model to predict user preferences. (Right) Success rates for personalized LLMs trained via CURIO with potential-based accuracy (DiffAcc) vs baseline RL.</figcaption></figure><h3 id=takeaways>Takeaways<a hidden class=anchor aria-hidden=true href=#takeaways>#</a></h3><ul><li><p>Reinforcement learning can effectively improve personalization beyond SFT on personalization data.
While this isn&rsquo;t news, it is good to see this borne out in the data and on the authors&rsquo; LLM-simulator setup.</p></li><li><p><strong>Intrinsic rewards improve learning efficiency and efficacy</strong> when applying RL for personalization with LLMs.
Particularly when there is ground-truth information about users, applying auxiliary objectives that encourage the LLM to produce conversations that reveal user preferences can lead to improved personalization.</p></li><li><p>Naively improving personalization performance can lead to drops in conversation quality.
The authors find that <strong>several versions of their intrinsic rewards lead to reward-hacking behaviors</strong> such as arbitrary lengthening of responses, mode collapse to a single preference type, or &ldquo;persuasion&rdquo; behaviors (where the LLM tries to change the user&rsquo;s preference rather than infer it). Looking through the appendix, it seems like the Environment Model (the LLM playing a synthetic human) tends to be very&mldr; LLM-y in its responses, leading to poor personalization performance and bad overall conversations, as the synthetic user opts to agree with suggestions and mindlessly agrees with everything that the Policy Model suggests, rather than behaving more like a real user.</p></li></ul><h3 id=thoughts--future-work>Thoughts & Future work<a hidden class=anchor aria-hidden=true href=#thoughts--future-work>#</a></h3><p>I like the idea of adding some auxiliary objective to the LLM that focuses on the conversation as a whole, rather than just on the end-product.
While the motivation of learning without labeled datasets didn&rsquo;t entirely match the methods (which assumed access to labeled human preferences), I think the findings and lessons are valuable for future research, and there are plenty of avenues opened up by this work.</p><p>First, it would be interesting to explore how we might relax the assumptions that we must know a user&rsquo;s preferences. Future work might explore more ideas like the entropy-based rewards in this work, where the LLM is encouraged to &ldquo;find out more&rdquo;, even if it doesn&rsquo;t know exactly what it&rsquo;s looking for.</p><p>Separately, it may be interesting to explore adding other objectives that improve the conversation quality or that help to mitigate reward hacking. For example, we could add dense rewards for conversations that are particularly informative, engaging, or thought-provoking, in addition to those that help reveal hidden preferences.</p><p>Finally, I appreciate the investment in modeling personalization as a multi-turn problem, in which the LLM has a chance to &ldquo;talk&rdquo; to a user before making a suggestion. Even so, this work assumes that users are static, and that preferences do not change with changing context (location, time-of-day, mood, etc.) or evolve over time. I&rsquo;m interested in seeing future work that considers how preferences evolve over time or in response to changing settings.</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>[1] Wan, Yanming, Jiaxing Wu, Marwa Abdulhai, Lior Shani, and Natasha Jaques. &ldquo;Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward.&rdquo; arXiv preprint arXiv:2504.03206 (2025). <a href=https://arxiv.org/pdf/2504.03206>https://arxiv.org/pdf/2504.03206</a></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://andrew-silva.github.io/>Andrew Writing Things</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>