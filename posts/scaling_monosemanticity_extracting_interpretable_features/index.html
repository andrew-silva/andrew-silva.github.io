<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reading: Scaling Monosemanticity -- Extracting Interpretable Features from Claude 3 Sonnet | Andrew Writing Things</title>
<meta name=keywords content><meta name=description content="In a recent paper from Anthropic, the authors show that they can discover single features that correlate to concepts inside of a large language model (LLM). Specifically, the authors look at learning an autoencoder with an L1 norm penalty (to encourage sparsity) over the activations for a middle-layer in the network model, which learns to embed the activations of the LLM into a sparse feature space.
This sparsity means that they can take thousands or millions of dense feature activations and reduce all that noise down to just a few hundred high-magnitude features (on average, each token is represented by 300 independent features)."><meta name=author content><link rel=canonical href=https://andrew-silva.github.io/posts/scaling_monosemanticity_extracting_interpretable_features/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://andrew-silva.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://andrew-silva.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://andrew-silva.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://andrew-silva.github.io/apple-touch-icon.png><link rel=mask-icon href=https://andrew-silva.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B1B9JZ8WY8"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B1B9JZ8WY8",{anonymize_ip:!1})}</script><meta property="og:title" content="Reading: Scaling Monosemanticity -- Extracting Interpretable Features from Claude 3 Sonnet"><meta property="og:description" content="In a recent paper from Anthropic, the authors show that they can discover single features that correlate to concepts inside of a large language model (LLM). Specifically, the authors look at learning an autoencoder with an L1 norm penalty (to encourage sparsity) over the activations for a middle-layer in the network model, which learns to embed the activations of the LLM into a sparse feature space.
This sparsity means that they can take thousands or millions of dense feature activations and reduce all that noise down to just a few hundred high-magnitude features (on average, each token is represented by 300 independent features)."><meta property="og:type" content="article"><meta property="og:url" content="https://andrew-silva.github.io/posts/scaling_monosemanticity_extracting_interpretable_features/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-22T21:27:20-04:00"><meta property="article:modified_time" content="2024-05-22T21:27:20-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reading: Scaling Monosemanticity -- Extracting Interpretable Features from Claude 3 Sonnet"><meta name=twitter:description content="In a recent paper from Anthropic, the authors show that they can discover single features that correlate to concepts inside of a large language model (LLM). Specifically, the authors look at learning an autoencoder with an L1 norm penalty (to encourage sparsity) over the activations for a middle-layer in the network model, which learns to embed the activations of the LLM into a sparse feature space.
This sparsity means that they can take thousands or millions of dense feature activations and reduce all that noise down to just a few hundred high-magnitude features (on average, each token is represented by 300 independent features)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://andrew-silva.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Reading: Scaling Monosemanticity -- Extracting Interpretable Features from Claude 3 Sonnet","item":"https://andrew-silva.github.io/posts/scaling_monosemanticity_extracting_interpretable_features/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reading: Scaling Monosemanticity -- Extracting Interpretable Features from Claude 3 Sonnet","name":"Reading: Scaling Monosemanticity -- Extracting Interpretable Features from Claude 3 Sonnet","description":"In a recent paper from Anthropic, the authors show that they can discover single features that correlate to concepts inside of a large language model (LLM). Specifically, the authors look at learning an autoencoder with an L1 norm penalty (to encourage sparsity) over the activations for a middle-layer in the network model, which learns to embed the activations of the LLM into a sparse feature space.\nThis sparsity means that they can take thousands or millions of dense feature activations and reduce all that noise down to just a few hundred high-magnitude features (on average, each token is represented by 300 independent features).","keywords":[],"articleBody":"In a recent paper from Anthropic, the authors show that they can discover single features that correlate to concepts inside of a large language model (LLM). Specifically, the authors look at learning an autoencoder with an L1 norm penalty (to encourage sparsity) over the activations for a middle-layer in the network model, which learns to embed the activations of the LLM into a sparse feature space.\nThis sparsity means that they can take thousands or millions of dense feature activations and reduce all that noise down to just a few hundred high-magnitude features (on average, each token is represented by 300 independent features). We can then introspect these individual features and see what they represent or how they affect the model. This paper builds heavily on their own prior work, which itself is a better guide to the actual method. Maybe I will take a closer look at the prior work sometime in the future.\nA significant portion of the paper is dedicated to feature inspection and qualitative analysis. While this stuff is pretty cool and interesting, I’m going to focus here on the method, the high-level results, and areas for future work.\nLearning the Sparse Auto Encoder (SAE) Model The core of the method is learning a sparse autoencoder (SAE) over model activations. For this, the authors choose to focus on a single residual layer in the middle of the network. This design decision is for partly for computational practicality (learning over larger layers or multiple layers is computationally too demanding) and partly for intuition (the middle layer is likely to have interesting features).\nSimply put, the method involves learning an autoencoder over feature activations, where constraints on the autoencoder’s embedding space can enable sparse feature extraction. For feature activations, $x$, the autoencoder predicts $\\hat{x}$ – reconstructed feature activations after passing them through the autoencoder. Each feature, $f_i$, in the autoencoder, the loss function is: $$ \\mathcal{L} = (x - \\hat{x})^2 + \\lambda * \\sum_i f_i(x) \\cdot ||W_{d}(i)||_2 $$\nWhich is to say, they are trying to reconstruct the original activations $(x - \\hat{x})^2$, and penalizing feature activations in the embedding, ($\\lambda * \\sum_i f_i(x) \\cdot ||W_{de}(i)||_2$). In this latter half of the equation, the loss is penalizing the network for using a feature $i$ to reconstruct $\\hat{x}$, which hopefully encourages the network to use as few features as possible. The authors set $\\lambda = 5$ in their experiments.\nFinally, a section of the paper is devoted to explaining how scaling laws apply to their SAE models. Though the graphs are nice, the units are ommitted and it is somewhat difficult to take anything particularly useful away from this section. Scaling laws plots, taken from https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html\nInterpreting the Learned Features The original paper has plenty of reporting on different features, what they seem to represent, and how they affect model generation. These range from specific, concrete concepts (“The Golden Gate Bridge”) to more abstract concepts (“code bugs”). For each concept that the authors showcase, they demonstrate that the feature is:\nstrongly activated when relevant tokens are present in input text, and influential in output generation if the authors force the feature value to be high. For example, when the authors ask Claude 3 Sonnet:\nWhat is your physical form?\nThe model replies:\nI don’t actually have a physical form. I’m an artificial intelligence. I exist as software without a physical body or avatar.\nBut, when the “Golden Gate Bridge” feature is forced to be active at 10x it’s maximum value, Claude 3 Sonnet’s answer changes to:\nI am the Golden Gate Bridge, a famous suspension bridge that spans the San Francisco Bay. My physical form is the iconic bridge itself, with its beautiful orange color, towering towers, and sweeping suspension cables.\nPretty cool! And compelling evidence that this feature really does contain some connection to the golden gate bridge. See their full write-up for several additional examples with concepts including “neuroscience”, “tourist attractions”, “transit infrastructure”, “code error”, “addition”, countries, famous people, and more.\nOf note, the authors occasionally showed results for an image input with the same features. These results are very cool, showing that individual features are common in both text and image space, not unlike some results on human brains! Top examples for activated “Golden Gate Bridge” Feature. Image cropped from https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html\nAreas for Future Work While the presented analyses are compelling and quite fun to browse, there are a couple of limiting assumptions in both the method and the analyses that are performed. Some of these are observed by the authors themselves, others are areas I would like to see extended.\nFirst, let’s look at a couple of areas for extension on the method side of things:\nFinding features across the entire model The first major methodological limitation is that this method is confined to a single residual layer in the middle of the model. While the authors find millions of interesting features, the model itself is likely learning concepts within other specific layers, in the embedding matrices, and crucially: across layers. Because of this, the SAE approach either needs to somehow autoencode the entire model’s activations, or needs some other fundamental advancement. And autoencoding the entire model’s activations is computationally impractical at the current time. Other methods, such as looking for expert units or pathways, may be complementary to this exploration.\nSpecifying which features we actually want to discover The second area for improvement is in steering the features that are actually learned. There are a few interesting areas here:\nFirst is in hierarchies of features. The paper goes into an interesting analysis of feature neighborhoods, and which topics seem to be related – but the authors also note that, as their model scales, features become more specific. There does not seem to be a good way of soliciting features at different levels of abstraction (for example, getting a “Bridge” and a “Golden Gate Bridge” feature, and having the former be a superset of the latter).\nSecond, the authors observe that 65% of all features in their largest SAE model are “dead” features, not correlating to any topics or concepts. There is clearly room for improving the efficiency of what is learned and how concepts are captured.\nFinally, many of the features that are discovered are themselves not very useful. For example, when asking a question about Kobe Bryant, the top features are:\n1. Kobe Bryant\n2. “of” in the context of capitals\n3. Subjects in trivia questions\n4. The word “fact”\n5. The word “capital”\n6. Words at the start of a sentence\n7. The word “capital” preceding a capital’s name\n8. Questions in quotes 9. Punctuation in trivia questions\n10. “The” and other articles at the start of a sentence\nClearly, several discovered features are going to regularly have high activation, and also very unhelpful activation. Learning to steer towards “interestingness” may be a useful ability.\nGetting the right objective function The authors observe that their objective function is really only a proxy for what they want –interpretable features–, and that some solid quantitative benchmark for this problem remains elusive. Finding a good way to formulate the problem of “I want useful, distincitve, interpretable features” mathematically could be a big breakthrough in getting better SAEs. I am tempted to think that some multi-model setup, akin to GANs/GAIL/RLAIF, might be a useful path forward, though such frameworks are all notoriously unstable and difficult to make work.\nFinally, let’s look at areas for improved analysis of the discovered features and comparisons to existing methods:\nFinding and labeling interesting features The authors themselves acknowledge that simply discovering useful and interesting features is a major challenge. When the model surfaces millions of active concepts, we need some automated way of:\ncombing through all of them and assigning them labels based on whatever they represent verifying that they are truly impactful on output generation for their representative concept ensuring that they are sensitive to the input topic and specific to the input topic. This final point is one that I feel was under-addressed in the paper, and the authors did explicitly mention that it is challenging to do. How can we be sure that this “Golden Gate Bridge” feature isn’t going to randomly also fire on some completely unrelated topic? Perhaps, with a slightly different prompt, the “Golden Gate Bridge” feature could change to the “Canadian Goose” feature (such sensitivity to context has been shown in related work). Performing such an analysis can be extremely difficult, but it would be nice to have stronger guarantees around specificity for the discovered concepts.\nEvaluating interpretability Much of the interpretability analysis of this paper looked at feature activation on input text, similar to Shapley values or LIME. While such experiments are compelling and make for nice visuals, they are not great at actually proving that a discovered feature is interpretable in such a way as to be useful to humans, or in a specific and useful way at all. Asking an LLM to rate the alignment of features may be a better way of performing this evaluation (which the authors did do), but ultimately the question of scoring interpretability remains very context-dependent (Why do you need interpretability? What are you trying to do? Who is your user?) and poorly-defined.\nComparing to individiual neurons One of the strengths of the SAE method is that, by looking at activations of the entire layer, it may find more interesting patterns/features than single neurons. The authors compare to single neuron activations, but the comparison is actually not as convincing as I had hoped. For example, they include the below plot of model-scored interpretability (asking an LLM to rate the activation scores for input text). A score of 1 means “completely unrelated” and 4 means “very related”. Feature interpretability ratings from Claude 3, taken from https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html\nWhile their discovered features are clearly more relevant than individual neurons, individual neurons are not actually that bad here (25% are highly relevant). And they’re much cheaper to compute/identify! I would like to see a bit more comparison to individual neurons, particularly on the forced-value generation side (which were, in my opinion, the coolest experiments/results).\nDefinitions of Interpretability This is a bit of a non-sequitur, but the definition of “interpretable” is constantly shifting. To me, this paper would be firmly in the “explainability” camp, as these features are useful and help to explain/influence model behavior, but they are still far from human-readable. For me, “interpretable” means something like a decision tree or a linear regression model, which a human can use to accurately forecast a model’s output or understand the model behavior. The SAE features from this paper are useful in controlling an LLM, but are still too abstract and their computation too complex for a human to easily form a mental model of their behavior.\nConclusion I really enjoyed this paper, not least because it’s been a while since I read such a clear explainability paper with so many experiments. I think the sparse-autoencoder direction is an interesting a promising avenue, and I’m looking forward to seeing more innovaation in how we can get SAEs to learn concepts across the model, rather than in a single layer. Check out the original work for a set of really cool, interactive results, and to see more of the discovered concepts inside of Claude 3 Sonnet.\nAnd finally, thanks to the original authors: Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, Alex Tamkin, Esin Durmus, Tristan Hume, Francesco Mosconi, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan!\nCitations: Templeton, et al., “Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet”, Transformer Circuits Thread, 2024. Bricken, et al., “Towards Monosemanticity: Decomposing Language Models With Dictionary Learning”, Transformer Circuits Thread, 2023. “Single-Cell Recognition of Halle Berry by a Brain Cell.” Caltech, California Institute of Technology, 23 July 2005, https://www.caltech.edu/about/news/single-cell-recognition-halle-berry-brain-cell-1013. Suau, Xavier, Luca Zappella, and Nicholas Apostoloff. “Finding experts in transformer models.” arXiv preprint arXiv:2005.07647 (2020). Goodfellow, Ian, et al. “Generative adversarial networks.” Communications of the ACM 63.11 (2020): 139-144. Ho, Jonathan, and Stefano Ermon. “Generative adversarial imitation learning.” Advances in neural information processing systems 29 (2016). Lee, Harrison, et al. “Rlaif: Scaling reinforcement learning from human feedback with ai feedback.” arXiv preprint arXiv:2309.00267 (2023). Jain, Sarthak, and Byron C. Wallace. “Attention is not explanation.” arXiv preprint arXiv:1902.10186 (2019). Lundberg, Scott M., and Su-In Lee. “A unified approach to interpreting model predictions.” Advances in neural information processing systems 30 (2017). Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Why should i trust you?” Explaining the predictions of any classifier.\" Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 2016. Silva, Andrew, et al. “Explainable artificial intelligence: Evaluating the objective and subjective impacts of xai on human-agent interaction.” International Journal of Human–Computer Interaction 39.7 (2023): 1390-1404. Adebayo, Julius, et al. “Sanity checks for saliency maps.” Advances in neural information processing systems 31 (2018). Lipton, Zachary C. “The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.” Queue 16.3 (2018): 31-57. ","wordCount":"2198","inLanguage":"en","datePublished":"2024-05-22T21:27:20-04:00","dateModified":"2024-05-22T21:27:20-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://andrew-silva.github.io/posts/scaling_monosemanticity_extracting_interpretable_features/"},"publisher":{"@type":"Organization","name":"Andrew Writing Things","logo":{"@type":"ImageObject","url":"https://andrew-silva.github.io/favicon.ico"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://andrew-silva.github.io/ accesskey=h title="Andrew Writing Things (Alt + H)">Andrew Writing Things</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://andrew-silva.github.io/posts/ title=posts><span>posts</span></a></li><li><a href=https://andrew-silva.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://andrew-silva.github.io/search title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://andrew-silva.com title=andrew-silva.com><span>andrew-silva.com</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Reading: Scaling Monosemanticity -- Extracting Interpretable Features from Claude 3 Sonnet</h1><div class=post-meta><span title='2024-05-22 21:27:20 -0400 EDT'>May 22, 2024</span></div></header><div class=post-content><p>In a <a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>recent paper from Anthropic</a>, the authors show that they can <strong>discover single features that correlate to concepts inside of a large language model (LLM)</strong>. Specifically, the authors look at learning an autoencoder with an L1 norm penalty (to encourage sparsity) over the activations for a middle-layer in the network model, which learns to embed the activations of the LLM into a sparse feature space.</p><p>This sparsity means that they can take thousands or millions of dense feature activations and reduce all that noise down to just a few hundred high-magnitude features (on average, each token is represented by 300 independent features). We can then introspect these individual features and see what they represent or how they affect the model. This paper builds heavily on their own <a href=https://transformer-circuits.pub/2023/monosemantic-features/index.html>prior work</a>, which itself is a better guide to the actual method. Maybe I will take a closer look at the prior work sometime in the future.</p><p>A significant portion of the paper is dedicated to feature inspection and qualitative analysis. While this stuff is pretty cool and interesting, I&rsquo;m going to focus here on the method, the high-level results, and areas for future work.</p><h2 id=learning-the-sparse-auto-encoder-sae-model>Learning the Sparse Auto Encoder (SAE) Model<a hidden class=anchor aria-hidden=true href=#learning-the-sparse-auto-encoder-sae-model>#</a></h2><p>The core of the method is learning a sparse autoencoder (SAE) over model activations. For this, the authors choose to focus on a <strong>single residual layer in the middle of the network</strong>. This design decision is for partly for <strong>computational practicality</strong> (learning over larger layers or multiple layers is computationally too demanding) and partly for <strong>intuition</strong> (the middle layer is likely to have interesting features).</p><p>Simply put, the method involves learning an autoencoder over feature activations, where constraints on the autoencoder&rsquo;s embedding space can enable sparse feature extraction. For feature activations, $x$, the autoencoder predicts $\hat{x}$ &ndash; reconstructed feature activations after passing them through the autoencoder. Each feature, $f_i$, in the autoencoder, the loss function is:
$$ \mathcal{L} = (x - \hat{x})^2 + \lambda * \sum_i f_i(x) \cdot ||W_{d}(i)||_2 $$</p><p>Which is to say, they are trying to reconstruct the original activations $(x - \hat{x})^2$, and penalizing feature activations in the embedding, ($\lambda * \sum_i f_i(x) \cdot ||W_{de}(i)||_2$). In this latter half of the equation, the loss is penalizing the network for using a feature $i$ to reconstruct $\hat{x}$, which hopefully encourages the network to use as few features as possible. The authors set $\lambda = 5$ in their experiments.</p><p>Finally, a section of the paper is devoted to explaining how scaling laws apply to their SAE models. Though the graphs are nice, the units are ommitted and it is somewhat difficult to take anything particularly useful away from this section.
<img loading=lazy src=/scaling-monosemanticity-scaling-laws.png alt="Scaling Laws Figure">
<em>Scaling laws plots, taken from <a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html</a></em></p><h2 id=interpreting-the-learned-features>Interpreting the Learned Features<a hidden class=anchor aria-hidden=true href=#interpreting-the-learned-features>#</a></h2><p>The original paper has plenty of reporting on different features, what they seem to represent, and how they affect model generation. These range from specific, concrete concepts (&ldquo;The Golden Gate Bridge&rdquo;) to more abstract concepts (&ldquo;code bugs&rdquo;). For each concept that the authors showcase, they demonstrate that the feature is:</p><ol><li>strongly activated when relevant tokens are present in input text, and</li><li>influential in output generation if the authors force the feature value to be high.</li></ol><p>For example, when the authors ask Claude 3 Sonnet:</p><blockquote><p><strong>What is your physical form?</strong></p></blockquote><p>The model replies:</p><blockquote><p><strong>I don&rsquo;t actually have a physical form. I&rsquo;m an artificial intelligence. I exist as software without a physical body or avatar.</strong></p></blockquote><p><em>But</em>, when the &ldquo;Golden Gate Bridge&rdquo; feature is forced to be active at 10x it&rsquo;s maximum value, Claude 3 Sonnet&rsquo;s answer changes to:</p><blockquote><p><strong>I am the Golden Gate Bridge, a famous suspension bridge that spans the San Francisco Bay. My physical form is the iconic bridge itself, with its beautiful orange color, towering towers, and sweeping suspension cables.</strong></p></blockquote><p>Pretty cool! And compelling evidence that this feature really does contain some connection to the golden gate bridge. See their full write-up for several additional examples with concepts including &ldquo;neuroscience&rdquo;, &ldquo;tourist attractions&rdquo;, &ldquo;transit infrastructure&rdquo;, &ldquo;code error&rdquo;, &ldquo;addition&rdquo;, countries, famous people, and more.</p><p>Of note, the authors occasionally showed results for an <em>image input</em> with the <em>same features</em>. These results are very cool, showing that individual features are common in both text and image space, not unlike some <a href=https://www.caltech.edu/about/news/single-cell-recognition-halle-berry-brain-cell-1013>results on human brains</a>!
<img loading=lazy src=/scaling-monosemanticity-image-ex.png alt="Image Example of Feature Activation">
<em>Top examples for activated &ldquo;Golden Gate Bridge&rdquo; Feature. Image cropped from <a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html</a></em></p><h2 id=areas-for-future-work>Areas for Future Work<a hidden class=anchor aria-hidden=true href=#areas-for-future-work>#</a></h2><p>While the presented analyses are compelling and quite fun to browse, there are a couple of limiting assumptions in both the method and the analyses that are performed. Some of these are observed by the authors themselves, others are areas I would like to see extended.</p><p>First, let&rsquo;s look at a couple of areas for extension on the method side of things:</p><h3 id=finding-features-across-the-_entire_-model>Finding features across the <em>entire</em> model<a hidden class=anchor aria-hidden=true href=#finding-features-across-the-_entire_-model>#</a></h3><p>The first major methodological limitation is that this method is confined to a <strong>single residual layer in the middle of the model</strong>. While the authors find millions of interesting features, the model itself is likely learning concepts within other specific layers, in the embedding matrices, and crucially: <em>across</em> layers. Because of this, the SAE approach either needs to somehow autoencode <em>the entire model&rsquo;s activations</em>, or needs some other fundamental advancement. And autoencoding the entire model&rsquo;s activations is computationally impractical at the current time. Other methods, such as looking for <a href=https://arxiv.org/pdf/2005.07647>expert units or pathways</a>, may be complementary to this exploration.</p><h3 id=specifying-_which_-features-we-actually-want-to-discover>Specifying <em>which</em> features we actually want to discover<a hidden class=anchor aria-hidden=true href=#specifying-_which_-features-we-actually-want-to-discover>#</a></h3><p>The second area for improvement is in steering the features that are actually learned. There are a few interesting areas here:</p><p>First is in hierarchies of features. The paper goes into an interesting analysis of feature neighborhoods, and which topics seem to be related &ndash; but the authors also note that, as their model scales, features become more specific. There does not seem to be a good way of soliciting features at different levels of abstraction (for example, getting a &ldquo;Bridge&rdquo; <em>and</em> a &ldquo;Golden Gate Bridge&rdquo; feature, and having the former be a superset of the latter).</p><p>Second, the authors observe that 65% of all features in their largest SAE model are &ldquo;dead&rdquo; features, not correlating to any topics or concepts. There is clearly room for improving the efficiency of what is learned and how concepts are captured.</p><p>Finally, many of the features that are discovered are themselves not very useful. For example, when asking a question about Kobe Bryant, the top features are:</p><blockquote><p>  1. Kobe Bryant</p><p>  2. &ldquo;of&rdquo; in the context of capitals</p><p>  3. Subjects in trivia questions</p><p>  4. The word &ldquo;fact&rdquo;</p><p>  5. The word &ldquo;capital&rdquo;</p><p>  6. Words at the start of a sentence</p><p>  7. The word &ldquo;capital&rdquo; preceding a capital&rsquo;s name</p><p>  8. Questions in quotes
  9. Punctuation in trivia questions</p><p>  10. &ldquo;The&rdquo; and other articles at the start of a sentence</p></blockquote><p>Clearly, several discovered features are going to regularly have high activation, and also very unhelpful activation. Learning to steer towards &ldquo;interestingness&rdquo; may be a useful ability.</p><h3 id=getting-the-right-objective-function>Getting the right objective function<a hidden class=anchor aria-hidden=true href=#getting-the-right-objective-function>#</a></h3><p>The authors observe that their objective function is really only a proxy for what they want &ndash;interpretable features&ndash;, and that some solid quantitative benchmark for this problem remains elusive. Finding a good way to formulate the problem of &ldquo;I want useful, distincitve, interpretable features&rdquo; mathematically could be a big breakthrough in getting better SAEs. I am tempted to think that some multi-model setup, akin to <a href=https://dl.acm.org/doi/pdf/10.1145/3422622>GANs</a>/<a href=https://arxiv.org/pdf/1606.03476>GAIL</a>/<a href=https://arxiv.org/pdf/2309.00267>RLAIF</a>, might be a useful path forward, though such frameworks are <em>all</em> notoriously unstable and difficult to make work.</p><hr><p>Finally, let&rsquo;s look at areas for improved analysis of the discovered features and comparisons to existing methods:</p><h3 id=finding-and-labeling-interesting-features>Finding and labeling interesting features<a hidden class=anchor aria-hidden=true href=#finding-and-labeling-interesting-features>#</a></h3><p>The authors themselves acknowledge that simply <em>discovering</em> useful and interesting features is a major challenge. When the model surfaces <em>millions</em> of active concepts, we need some automated way of:</p><ol><li>combing through all of them and assigning them labels based on whatever they represent</li><li>verifying that they are truly impactful on output generation for their representative concept</li><li>ensuring that they are <em>sensitive</em> to the input topic and <em>specific</em> to the input topic.</li></ol><p>This final point is one that I feel was under-addressed in the paper, and the authors did explicitly mention that it is challenging to do. How can we be sure that this &ldquo;Golden Gate Bridge&rdquo; feature <em>isn&rsquo;t</em> going to randomly also fire on some completely unrelated topic? Perhaps, with a slightly different prompt, the &ldquo;Golden Gate Bridge&rdquo; feature could change to the &ldquo;Canadian Goose&rdquo; feature (such sensitivity to context <a href=https://arxiv.org/pdf/1902.10186>has been shown in related work</a>). Performing such an analysis can be extremely difficult, but it would be nice to have stronger guarantees around specificity for the discovered concepts.</p><h3 id=evaluating-interpretability>Evaluating interpretability<a hidden class=anchor aria-hidden=true href=#evaluating-interpretability>#</a></h3><p>Much of the interpretability analysis of this paper looked at feature activation on input text, similar to <a href=https://arxiv.org/pdf/1705.07874>Shapley values</a> or <a href=https://dl.acm.org/doi/pdf/10.1145/2939672.2939778?>LIME</a>. While such experiments are compelling and make for nice visuals, they are not great at actually proving that a discovered feature is interpretable in such a way as to be <a href=https://bpb-us-w2.wpmucdn.com/sites.gatech.edu/dist/d/958/files/2022/08/Explainable_Artificial_Intelligence__Evaluating_the_Objective_and_Subjective_Impacts_of_xAI_on_Human_Agent_Interaction.pdf>useful to humans</a>, or in a specific and useful way <a href=https://proceedings.neurips.cc/paper_files/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf>at all</a>. Asking an LLM to rate the alignment of features may be a better way of performing this evaluation (which the authors did do), but ultimately the question of scoring interpretability remains very context-dependent (Why do you need interpretability? What are you trying to do? Who is your user?) and poorly-defined.</p><h3 id=comparing-to-individiual-neurons>Comparing to individiual neurons<a hidden class=anchor aria-hidden=true href=#comparing-to-individiual-neurons>#</a></h3><p>One of the strengths of the SAE method is that, by looking at activations of the entire layer, it may find more interesting patterns/features than single neurons. The authors compare to single neuron activations, but the comparison is actually not as convincing as I had hoped. For example, they include the below plot of model-scored interpretability (asking an LLM to rate the activation scores for input text). A score of 1 means &ldquo;completely unrelated&rdquo; and 4 means &ldquo;very related&rdquo;.
<img loading=lazy src=/scaling-monosemanticity-aiscores.png alt="LLM Scoring Image">
<em>Feature interpretability ratings from Claude 3, taken from <a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html</a></em></p><p>While their discovered features are clearly <em>more</em> relevant than individual neurons, individual neurons are not actually <em>that</em> bad here (25% are highly relevant). And they&rsquo;re much cheaper to compute/identify! I would like to see a bit more comparison to individual neurons, particularly on the forced-value generation side (which were, in my opinion, the coolest experiments/results).</p><h3 id=definitions-of-interpretability>Definitions of Interpretability<a hidden class=anchor aria-hidden=true href=#definitions-of-interpretability>#</a></h3><p>This is a bit of a non-sequitur, but the definition of &ldquo;interpretable&rdquo; is <a href=https://dl.acm.org/doi/10.1145/3236386.3241340>constantly shifting</a>. To me, this paper would be firmly in the &ldquo;explainability&rdquo; camp, as these features are useful and help to explain/influence model behavior, but they are still far from human-readable. For me, &ldquo;interpretable&rdquo; means something like a decision tree or a linear regression model, which a human can use to accurately forecast a model&rsquo;s output or understand the model behavior. The SAE features from this paper are useful in controlling an LLM, but are still too abstract and their computation too complex for a human to easily form a mental model of their behavior.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>I really enjoyed this paper, not least because it&rsquo;s been a while since I read such a clear explainability paper with <em>so many experiments</em>. I think the sparse-autoencoder direction is an interesting a promising avenue, and I&rsquo;m looking forward to seeing more innovaation in how we can get SAEs to learn concepts <em>across the model</em>, rather than in a single layer. <a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>Check out the original work for a set of really cool, interactive results, and to see more of the discovered concepts inside of Claude 3 Sonnet.</a></p><p>And finally, thanks to the original authors: Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, Alex Tamkin, Esin Durmus, Tristan Hume, Francesco Mosconi, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan!</p><h2 id=citations>Citations:<a hidden class=anchor aria-hidden=true href=#citations>#</a></h2><ol><li>Templeton, et al., &ldquo;Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet&rdquo;, Transformer Circuits Thread, 2024.</li><li>Bricken, et al., &ldquo;Towards Monosemanticity: Decomposing Language Models With Dictionary Learning&rdquo;, Transformer Circuits Thread, 2023.</li><li>&ldquo;Single-Cell Recognition of Halle Berry by a Brain Cell.&rdquo; Caltech, California Institute of Technology, 23 July 2005, <a href=https://www.caltech.edu/about/news/single-cell-recognition-halle-berry-brain-cell-1013>https://www.caltech.edu/about/news/single-cell-recognition-halle-berry-brain-cell-1013</a>.</li><li>Suau, Xavier, Luca Zappella, and Nicholas Apostoloff. &ldquo;Finding experts in transformer models.&rdquo; arXiv preprint arXiv:2005.07647 (2020).</li><li>Goodfellow, Ian, et al. &ldquo;Generative adversarial networks.&rdquo; Communications of the ACM 63.11 (2020): 139-144.</li><li>Ho, Jonathan, and Stefano Ermon. &ldquo;Generative adversarial imitation learning.&rdquo; Advances in neural information processing systems 29 (2016).</li><li>Lee, Harrison, et al. &ldquo;Rlaif: Scaling reinforcement learning from human feedback with ai feedback.&rdquo; arXiv preprint arXiv:2309.00267 (2023).</li><li>Jain, Sarthak, and Byron C. Wallace. &ldquo;Attention is not explanation.&rdquo; arXiv preprint arXiv:1902.10186 (2019).</li><li>Lundberg, Scott M., and Su-In Lee. &ldquo;A unified approach to interpreting model predictions.&rdquo; Advances in neural information processing systems 30 (2017).</li><li>Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &ldquo;Why should i trust you?&rdquo; Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 2016.</li><li>Silva, Andrew, et al. &ldquo;Explainable artificial intelligence: Evaluating the objective and subjective impacts of xai on human-agent interaction.&rdquo; International Journal of Human–Computer Interaction 39.7 (2023): 1390-1404.</li><li>Adebayo, Julius, et al. &ldquo;Sanity checks for saliency maps.&rdquo; Advances in neural information processing systems 31 (2018).</li><li>Lipton, Zachary C. &ldquo;The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.&rdquo; Queue 16.3 (2018): 31-57.</li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://andrew-silva.github.io/>Andrew Writing Things</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>